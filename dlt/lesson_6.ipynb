{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does `dlt` Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `dlt` Pipeline\n",
    "- main building block of `dlt` pipeline is the class `pipeline`\n",
    "- `pipeline.run()` method encompasses 3 steps : extract, normalise, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------- Extract my ----------------------------------\n",
      "Resources: 0/1 (0.0%) | Time: 0.00s | Rate: 0.00/s\n",
      "Memory usage: 170.01 MB (47.30%) | CPU usage: 0.00%\n",
      "\n",
      "---------------------------------- Extract my ----------------------------------\n",
      "Resources: 0/1 (0.0%) | Time: 0.02s | Rate: 0.00/s\n",
      "items: 1  | Time: 0.00s | Rate: 0.00/s\n",
      "Memory usage: 170.27 MB (47.30%) | CPU usage: 0.00%\n",
      "\n",
      "---------------------------------- Extract my ----------------------------------\n",
      "Resources: 1/1 (100.0%) | Time: 0.03s | Rate: 29.62/s\n",
      "items: 3  | Time: 0.02s | Rate: 179.14/s\n",
      "Memory usage: 170.37 MB (47.30%) | CPU usage: 0.00%\n",
      "\n",
      "---------------------- Normalize my in 1740475667.9840188 ----------------------\n",
      "Files: 0/1 (0.0%) | Time: 0.00s | Rate: 0.00/s\n",
      "Memory usage: 170.69 MB (47.30%) | CPU usage: 0.00%\n",
      "\n",
      "---------------------- Normalize my in 1740475667.9840188 ----------------------\n",
      "Files: 0/1 (0.0%) | Time: 0.02s | Rate: 0.00/s\n",
      "Items: 0  | Time: 0.00s | Rate: 0.00/s\n",
      "Memory usage: 170.69 MB (47.30%) | CPU usage: 0.00%\n",
      "\n",
      "---------------------- Normalize my in 1740475667.9840188 ----------------------\n",
      "Files: 2/1 (200.0%) | Time: 0.03s | Rate: 60.25/s\n",
      "Items: 5  | Time: 0.02s | Rate: 285.21/s\n",
      "Memory usage: 170.75 MB (47.30%) | CPU usage: 0.00%\n",
      "\n",
      "------------------------ Load my in 1740475667.9840188 -------------------------\n",
      "Jobs: 0/2 (0.0%) | Time: 0.00s | Rate: 0.00/s\n",
      "Memory usage: 170.87 MB (47.30%) | CPU usage: 0.00%\n",
      "\n",
      "------------------------ Load my in 1740475667.9840188 -------------------------\n",
      "Jobs: 1/2 (50.0%) | Time: 1.10s | Rate: 0.91/s\n",
      "Memory usage: 171.48 MB (47.30%) | CPU usage: 0.00%\n",
      "\n",
      "------------------------ Load my in 1740475667.9840188 -------------------------\n",
      "Jobs: 2/2 (100.0%) | Time: 1.18s | Rate: 1.69/s\n",
      "Memory usage: 172.69 MB (47.30%) | CPU usage: 0.00%\n",
      "\n",
      "Pipeline my_pipeline load step completed in 1.17 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset mydata\n",
      "The duckdb destination used duckdb:///c:\\Users\\HP\\OneDrive\\Desktop\\Data Engg\\dlt\\my_pipeline.duckdb location to store data\n",
      "Load package 1740475667.9840188 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"my_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    "    progress=\"log\" # enables detailed logging\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(\n",
    "    [\n",
    "        {\"id\": 1},\n",
    "        {\"id\": 2},\n",
    "        {\"id\": 3, \"nested\": [{\"id\": 1}, {\"id\": 2}]},\n",
    "    ],\n",
    "    table_name=\"items\",\n",
    ")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Extract** - Fully extracts the data from source to your hard drive \n",
    "2. **Normalize** - Inspects data to commute a schema; Unnests nested fields\n",
    "3. **Load** - Loads data into destination; runs schema migration if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Stage `pipeline.extract(data)`\n",
    "\n",
    "1. Data is first collected in an in-memory buffer (default holds upto 5000 items) - it can be thought of as a bucket for storing data as it is loaded in\n",
    "2. When the bucket is full it is written to an intermediate temp file on your disk and then the bucket is emptied to collect more data\n",
    "3. If a size is specified for intermediary files and an the intermediary file in question reaches this size, a new intermediary file is opened for further data.\n",
    "\n",
    "\n",
    "### Default Behaviour\n",
    "- The in-memory buffer is set to 5000 items.\n",
    "- By default, intermediary files are not rotated. If you do not explicitly set a size for an intermediary file with file_max_items=100000, dlt will create a single file for a resource, regardless of the number of records it contains, even if it reaches millions.\n",
    "- By default, intermediary files at the extract stage use a custom version of the JSONL format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Stage `pipeline.normalize()`\n",
    "\n",
    "- It is dependent on extracted data\n",
    "- Intermediary files from `extract` stage are pushed forward to the `normalize` stage\n",
    "- One intermediary file is processed one at a time in it's own in-memory buffer.\n",
    "- Here as well: if buffer is full -> write the data to an intermediary file & clear buffer\n",
    "- If size of intermediary file reaches the specified threshold then a new file is opened (by default intermediary files are not rotated that means it will keep loading in 1 intermediary file itself)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stage `pipeline.load()`\n",
    "\n",
    "- Dependent on having completed the normalize stage \n",
    "- All intermediary files from a single source are combined into a single load package\n",
    "- All load packages are then loaded to the destination\n",
    "\n",
    "### Default behaviour\n",
    "- Loading happens in 20 threads, each loading a single file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediary File - Formats\n",
    "\n",
    "Intermediary files at the extract stage use a custom version of the JSONL format, while the loader files - files created at the normalize stage - can take 4 different formats.\n",
    "\n",
    "### **JSONL**\n",
    "- JSONL: JSON Delimited is a file format that stores several JSON documents in one file. The JSON documents are separated by a new line\n",
    "    - Used by: BigQuery, Snowflake, Filesystem\n",
    "\n",
    "#### Configuration\n",
    "Option 1 - specify in pipeline run\n",
    "`info = pipeline.run(some_source(), loader_file_format=\"jsonl\")`\n",
    "\n",
    "Option 2 - config.toml or secrets.toml\n",
    "```\n",
    "[normalize]\n",
    "loader_file_format=\"jsonl\"\n",
    "```\n",
    "\n",
    "Option 3 - Via ENV Variables\n",
    "`export NORMALIZE__LOADER_FILE_FORMAT=\"jsonl\"`\n",
    "\n",
    "\n",
    "Option 4 - Specify in resource decorator\n",
    "```python\n",
    "@dlt.resource(file_format='jsonl')\n",
    "def generate_rows():\n",
    "```\n",
    "\n",
    "\n",
    "### **Parquet**\n",
    "- Apache Parquet is a free and open-source column-oriented data storage format in the Apache Hadoop ecosystem.\n",
    "- To use this format, you need a pyarrow package. You can get this package as a dlt extra as well: `pip install \"dlt[parquet]\"`\n",
    "\n",
    "#### Configuration\n",
    "Option 1 - specify in pipeline run\n",
    "`info = pipeline.run(some_source(), loader_file_format=\"parquet\")`\n",
    "\n",
    "Option 2 - config.toml or secrets.toml\n",
    "```\n",
    "[normalize]\n",
    "loader_file_format=\"parquet\"\n",
    "```\n",
    "\n",
    "Option 3 - Via ENV Variables\n",
    "`export NORMALIZE__LOADER_FILE_FORMAT=\"parquet\"`\n",
    "\n",
    "\n",
    "Option 4 - Specify in resource decorator\n",
    "```python\n",
    "@dlt.resource(file_format='parquet')\n",
    "def generate_rows():\n",
    "```\n",
    "\n",
    "**Destination AutoConfig**:\n",
    "\n",
    "`dlt` automatically configures the Parquet writer based on the destination's capabilities:\n",
    "\n",
    "- Selects the appropriate decimal type and sets the correct precision and scale for accurate numeric data storage, including handling very small units like Wei.\n",
    "\n",
    "- Adjusts the timestamp resolution (seconds, microseconds, or nanoseconds) to match what the destination supports\n",
    "\n",
    "\n",
    "**Writer settings:**\n",
    "\n",
    "`dlt` uses the pyarrow Parquet writer for file creation. You can adjust the writer's behavior with the following options:\n",
    "\n",
    "- `flavor` adjusts schema and compatibility settings for different target systems. Defaults to None (pyarrow default).\n",
    "- `version` selects Parquet logical types based on the Parquet format version. Defaults to \"2.6\".\n",
    "- `data_page_size` sets the target size for data pages within a column chunk (in bytes). Defaults to None.\n",
    "- `timestamp_timezone` specifies the timezone; defaults to UTC.\n",
    "- `coerce_timestamps` sets the timestamp resolution (s, ms, us, ns).\n",
    "- `allow_truncated_timestamps` raises an error if precision is lost on truncated timestamps.\n",
    "\n",
    "  **Example configurations:**\n",
    "\n",
    "  - In `configs.toml` or `secrets.toml`:\n",
    "    ```py\n",
    "    [normalize.data_writer]\n",
    "    # the default values\n",
    "    flavor=\"spark\"\n",
    "    version=\"2.4\"\n",
    "    data_page_size=1048576\n",
    "    timestamp_timezone=\"Europe/Berlin\"\n",
    "    ```\n",
    "\n",
    "  - Via environment variables:\n",
    "    ```py\n",
    "    export  NORMALIZE__DATA_WRITER__FLAVOR=\"spark\"\n",
    "    ```\n",
    "\n",
    "\n",
    "**Timestamps and timezones**\n",
    "\n",
    "`dlt` adds UTC adjustments to all timestamps, creating timezone-aware timestamp columns in destinations (except DuckDB).\n",
    "\n",
    "**Disable timezone/UTC adjustments:**\n",
    "\n",
    "- Set `flavor` to spark to use the deprecated `int96` timestamp type without logical adjustments.\n",
    "\n",
    "- Set `timestamp_timezone` to an empty string (`DATA_WRITER__TIMESTAMP_TIMEZONE=\"\"`) to generate logical timestamps without UTC adjustment.\n",
    "\n",
    "By default, pyarrow converts timezone-aware DateTime objects to UTC and stores them in Parquet without timezone information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CSV**\n",
    "\n",
    "#### Configuration\n",
    "\n",
    "- Directly in the `pipeline.run()`:\n",
    "\n",
    "  ```py\n",
    "  info = pipeline.run(some_source(), loader_file_format=\"csv\")\n",
    "  ```\n",
    "\n",
    "- In `config.toml` or `secrets.toml`:\n",
    "\n",
    "  ```py\n",
    "  [normalize]\n",
    "  loader_file_format=\"csv\"\n",
    "  ```\n",
    "\n",
    "- Via environment variables:\n",
    "\n",
    "  ```py\n",
    "  export NORMALIZE__LOADER_FILE_FORMAT=\"csv\"\n",
    "  ```\n",
    "\n",
    "- Specify directly in the resource decorator:\n",
    "\n",
    "  ```py\n",
    "  @dlt.resource(file_format=\"csv\")\n",
    "  def generate_rows():\n",
    "    ...\n",
    "  ```\n",
    "\n",
    "\n",
    "**Two implementation**:\n",
    "\n",
    "1. `pyarrow` csv writer - very fast, multithreaded writer for the arrow tables\n",
    "  - binary columns are supported only if they contain valid UTF-8 characters\n",
    "  - complex (nested, struct) types are not supported\n",
    "2. `python stdlib writer` - a csv writer included in the Python standard library for Python objects\n",
    "\n",
    "  - binary columns are supported only if they contain valid UTF-8 characters (easy to add more encodings)\n",
    "  - complex columns dumped with json.dumps\n",
    "  - None values are always quoted\n",
    "\n",
    "**Default settings:**\n",
    "\n",
    "- separators are commas\n",
    "- quotes are \" and are escaped as \"\"\n",
    "- NULL values both are empty strings and empty tokens as in the example below\n",
    "- UNIX new lines are used\n",
    "- dates are represented as ISO 8601\n",
    "quoting style is \"when needed\"\n",
    "\n",
    "**Adjustable setting:**\n",
    "\n",
    "- `delimiter`: change the delimiting character (default: ',')\n",
    "- `include_header`: include the header row (default: True)\n",
    "- `quoting`: `quote_all` - all values are quoted, `quote_needed` - quote only values that need quoting (default: `quote_needed`)\n",
    "\n",
    "  ```py\n",
    "  [normalize.data_writer]\n",
    "  delimiter=\"|\"\n",
    "  include_header=false\n",
    "  quoting=\"quote_all\"\n",
    "  ```\n",
    "\n",
    "  or\n",
    "\n",
    "  ```py\n",
    "  NORMALIZE__DATA_WRITER__DELIMITER=|\n",
    "  NORMALIZE__DATA_WRITER__INCLUDE_HEADER=False\n",
    "  NORMALIZE__DATA_WRITER__QUOTING=quote_all\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SQL INSERT FILE FORMAT**\n",
    "- file contains INSERT VALUES statements to be executed on the destination\n",
    "\n",
    "#### Configuration\n",
    "\n",
    "- Directly in the `pipeline.run()`:\n",
    "\n",
    "  ```py\n",
    "  info = pipeline.run(some_source(), loader_file_format=\"insert_values\")\n",
    "  ```\n",
    "\n",
    "- In `config.toml` or `secrets.toml`:\n",
    "\n",
    "  ```py\n",
    "  [normalize]\n",
    "  loader_file_format=\"insert_values\"\n",
    "  ```\n",
    "\n",
    "- Via environment variables:\n",
    "\n",
    "  ```py\n",
    "  export NORMALIZE__LOADER_FILE_FORMAT=\"insert_values\"\n",
    "  ```\n",
    "\n",
    "- Specify directly in the resource decorator:\n",
    "\n",
    "  ```py\n",
    "  @dlt.resource(file_format=\"insert_values\")\n",
    "  def generate_rows():\n",
    "    ...\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
