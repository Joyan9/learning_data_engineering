{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Understanding Pipeline Metadata and State**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Metadata \n",
    "- Data about your data pipeline\n",
    "    - When your pipeline first ran\n",
    "    - When your pipeline last ran\n",
    "    - Information about your source or destination\n",
    "    - Processing time\n",
    "    - Or information that you yourself may want to add to the metadata\n",
    "\n",
    "- You can check the meta data through\n",
    "    - Load info\n",
    "    - trace\n",
    "    - state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline lesson_8 load step completed in 0.34 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset github_data\n",
      "The duckdb destination used duckdb:///c:\\Users\\HP\\OneDrive\\Desktop\\Data Engg\\dlt\\Lesson Notebooks\\lesson_8.duckdb location to store data\n",
      "Load package 1740566505.9691741 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "from dlt.sources.helpers import requests\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.auth import BearerTokenAuth\n",
    "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator\n",
    "import os\n",
    "\n",
    "os.environ['SOURCES__SECRET_KEY'] = os.getenv('GITHUB_TOKEN')\n",
    "\n",
    "@dlt.source\n",
    "def github_source(secret_key=dlt.secrets.value):\n",
    "    client = RESTClient(\n",
    "            base_url=\"https://api.github.com\",\n",
    "            auth=BearerTokenAuth(token=secret_key),\n",
    "            paginator=HeaderLinkPaginator(),\n",
    "    )\n",
    "\n",
    "    @dlt.resource\n",
    "    def github_pulls(cursor_date=dlt.sources.incremental(\"updated_at\", initial_value=\"2025-20-01\")):\n",
    "        params = {\n",
    "            \"since\": cursor_date.last_value,\n",
    "            \"status\": \"open\"\n",
    "        }\n",
    "        for page in client.paginate(\"repos/dlt-hub/dlt/pulls\", params=params):\n",
    "            yield page\n",
    "\n",
    "\n",
    "    return github_pulls\n",
    "\n",
    "\n",
    "# define new dlt pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"lesson_8\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"github_data\",\n",
    ")\n",
    "\n",
    "\n",
    "# run the pipeline with the new resource\n",
    "load_info = pipeline.run(github_source())\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load Info**\n",
    "\n",
    "`Load info`: collection of useful info for the recently loaded data - details like the pipeline and dataset name, destination information, and a list of loaded packages with their statuses, file sizes, types, and error messages (if any).\n",
    "\n",
    "`Load Package` : collection of jobs with specific data specific tables, generated during each execution of the pipeline. Each package is uniquely identified by a `load_id`.\n",
    "\n",
    "> The `load_id` of a particular package is added to the top data tables (parent tables) and to the special `_dlt_loads` table with a status of 0 when the load process is fully completed. The `_dlt_loads` table tracks complete loads and allows chaining transformations on top of them. We can also see load package info with a specific load id:\n",
    "\n",
    "`!dlt pipeline lesson_8 load-package 1734535481.3936043`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The package with load id 1740566505.9691741 for schema github_source is in LOADED state. It updated schema for 3 tables. The package was LOADED at 2025-02-26 10:41:47.804483+00:00.\n",
      "Jobs details:\n",
      "Job: _dlt_pipeline_state.e9f9555659.insert_values, table: _dlt_pipeline_state in completed_jobs. File type: insert_values, size: 586B. Started on: 2025-02-26 10:41:47.418064+00:00 and completed in 0.39 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(load_info.load_packages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute/Method                                   Type      \n",
      "----------------------------------------\n",
      "asdict                                             method    \n",
      "asstr                                              method    \n",
      "completed_at                                       attribute \n",
      "count                                              method    \n",
      "index                                              method    \n",
      "jobs                                               attribute \n",
      "load_id                                            attribute \n",
      "package_path                                       attribute \n",
      "schema                                             attribute \n",
      "schema_hash                                        attribute \n",
      "schema_name                                        attribute \n",
      "schema_update                                      attribute \n",
      "state                                              attribute \n"
     ]
    }
   ],
   "source": [
    "# This code snippet just prints out the public methods and attributes of the schema object in load info\n",
    "all_attributes_methods = dir(load_info.load_packages[0])\n",
    "public_attributes_methods = [attr for attr in all_attributes_methods if not attr.startswith('_')]\n",
    "\n",
    "print(f\"{'Attribute/Method':<50} {'Type':<10}\")\n",
    "print(\"-\" * 40)\n",
    "for attr in public_attributes_methods:\n",
    "    attr_value = getattr(load_info.load_packages[0], attr)\n",
    "    if callable(attr_value):\n",
    "        print(f\"{attr:<50} {'method':<10}\")\n",
    "    else:\n",
    "        print(f\"{attr:<50} {'attribute':<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Trace**\n",
    "\n",
    "- A trace is a detailed record of the execution of a pipeline. \n",
    "- It provides rich information on the pipeline processing steps: extract, normalize, and load. \n",
    "- It also shows the last load_info.\n",
    "\n",
    "### Accessing Trac\n",
    "\n",
    "**1. CLI**\n",
    "\n",
    "`!dlt pipeline <pipeline_name> trace`\n",
    "\n",
    "\n",
    "**2. Python**\n",
    "\n",
    "```python\n",
    "print(pipeline.last_trace)\n",
    "\n",
    "print(pipeline.last_trace.last_extract_info) # prints trace for extract stage\n",
    "\n",
    "print(pipeline.last_trace.last_normalize_info) # prints trace for normalize stage\n",
    "# access row counts dictionary of normalize info\n",
    "print(pipeline.last_trace.last_normalize_info.row_counts)\n",
    "\n",
    "print(pipeline.last_trace.last_load_info) # prints trace for load stage\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **State**\n",
    "\n",
    "- It's a Python dictionary that lives alongside the data\n",
    "- It can store values during the pipeline run and then retreive it for the next run\n",
    "- It's used for tasks like preserving the \"last value\" or similar loading checkpoints, and it gets committed atomically with the data\n",
    "- The state is stored locally in the pipeline working directory and is also stored at the destination for future runs\n",
    "- The pipeline state is stored locally in the pipeline working directory and, as a consequence, it cannot be shared with pipelines with different names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use pipeline state**\n",
    "- dlt uses the state internally to implement last value incremental loading. This use case should cover around 90% of your needs to use the pipeline state.\n",
    "- Store a list of already requested entities if the list is not much bigger than 100k elements.\n",
    "- Store large dictionaries of last values if you are not able to implement it with the standard incremental construct.\n",
    "- Store the custom fields dictionaries, dynamic configurations and other source-scoped state.\n",
    "\n",
    "**When not to use pipeline state**\n",
    "\n",
    "Do not use dlt state when it may grow to millions of elements. Do you plan to store modification timestamps of all of your millions of user records? This is probably a bad idea! In that case you could:\n",
    "\n",
    "- Store the state in dynamo-db, redis etc. taking into the account that if the extract stage fails you'll end with invalid state.\n",
    "- Use your loaded data as the state. dlt exposes the current pipeline via dlt.current.pipeline() from which you can obtain sqlclient and load the data of interest. In that case try at least to process your user records in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing State\n",
    "\n",
    "**1. CLI**\n",
    "\n",
    "`!dlt pipeline -v <pipeline_name> info`\n",
    "\n",
    "**2. Python**\n",
    "\n",
    "```python\n",
    "# helper function to read state\n",
    "import json\n",
    "\n",
    "def read_state(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        pretty_json = json.dumps(data, indent=4)\n",
    "        return pretty_json\n",
    "\n",
    "\n",
    "# stored in your default pipelines folder\n",
    "print(read_state(\"/var/dlt/pipelines/<pipeline_name>/state.json\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline lesson_8_state_pipeline load step completed in 0.94 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset github_data\n",
      "The duckdb destination used duckdb:///c:\\Users\\HP\\OneDrive\\Desktop\\Data Engg\\dlt\\Lesson Notebooks\\lesson_8_state_pipeline.duckdb location to store data\n",
      "Load package 1740569632.9830878 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "from dlt.sources.helpers import requests\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.auth import BearerTokenAuth\n",
    "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"SOURCES__SECRET_KEY\"] = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "\n",
    "@dlt.source\n",
    "def github_source(secret_key=dlt.secrets.value):\n",
    "    client = RESTClient(\n",
    "            base_url=\"https://api.github.com\",\n",
    "            auth=BearerTokenAuth(token=secret_key),\n",
    "            paginator=HeaderLinkPaginator(),\n",
    "    )\n",
    "\n",
    "    @dlt.resource\n",
    "    def github_pulls(cursor_date=dlt.sources.incremental(\"updated_at\", initial_value=\"2024-12-01\")):\n",
    "\n",
    "        # Let's set some custom state information\n",
    "        dlt.current.resource_state().setdefault(\"new_key\", [\"first_value\", \"second_value\"]) # <--- new item in the state\n",
    "\n",
    "        params = {\n",
    "            \"since\": cursor_date.last_value,\n",
    "            \"status\": \"open\"\n",
    "        }\n",
    "        for page in client.paginate(\"repos/dlt-hub/dlt/pulls\", params=params):\n",
    "            yield page\n",
    "\n",
    "\n",
    "    return github_pulls\n",
    "\n",
    "\n",
    "# define new dlt pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"lesson_8_state_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"github_data\",\n",
    ")\n",
    "\n",
    "\n",
    "# run the pipeline with the new resource\n",
    "load_info = pipeline.run(github_source())\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source scoped states\n",
    "\n",
    "You can access the source-scoped state with `dlt.current.source_state()` which can be shared across resources of a particular source and is also available **read-only** in the source-decorated functions. The most common use case for the source-scoped state is to store mapping of custom fields to their displayable names.\n",
    "\n",
    "\n",
    "Let's read some custom keys from the state:\n",
    "```python\n",
    "# Let's read some custom state information\n",
    "source_new_keys = dlt.current.source_state().get(\"resources\", {}).get(\"github_pulls\", {}).get(\"new_key\")\n",
    "```\n",
    "Full example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
